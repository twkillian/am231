{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ES 201 / AM231 Homework 1 (due 02/09/2017)\n",
    "### Reading : 2.1-2.3, 3, 6.1,6.2,6.5\n",
    "\n",
    "Completed by **Taylor Killian**\n",
    "\n",
    "Collaborators: Sophie Hilgard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "#### Let $X$ be full-rank $n \\times p$ matrix with $n > p$ Show that:\n",
    "\n",
    "##### a) $X^\\intercal X$ is positive definite\n",
    "\n",
    "Since $n < p$ and we are told that $X$ is full-rank we know that the matrix has full column-rank. That is, the columns of $X$ are linearly independent. So any right-multiplication between $X$ and a non-zero vector $v\\in\\mathbb{R}^p$ will be non-zero.\n",
    "\n",
    "Then, with any non-zero vector $ z\\in\\mathbb{R}^p$\n",
    "\\begin{eqnarray}\n",
    "z^\\intercal X^\\intercal Xz \\ &=& \\ (Xz)^\\intercal Xz \\\\\n",
    "\\ \\\\\n",
    "&=& \\|Xz\\|_2^2 > 0\n",
    "\\end{eqnarray}\n",
    "\n",
    "Thus $X^\\intercal X$ is positive definite $\\blacksquare$\n",
    "\n",
    "\n",
    "##### b) $XX^\\intercal$ is positive semi-definite\n",
    "\n",
    "Since the matrix X is overdetermined, the rows are not linearly independent. That is, there may exist some non-zero vector $v\\in\\mathbb{R}^n$ such that $X^\\intercal v = 0$.\n",
    "\n",
    "Then, with any vector $z\\in\\mathbb{R}^n$\n",
    "\\begin{eqnarray}\n",
    "z^\\intercal XX^\\intercal z \\ &=& \\ (X^\\intercal z)^\\intercal X^\\intercal z \\\\\n",
    "\\ \\\\\n",
    "&=& \\|X^\\intercal z\\|_2^2 \\geq 0\n",
    "\\end{eqnarray}\n",
    "\n",
    "Thus, $XX^\\intercal$ is positive semi-definite $\\blacksquare$\n",
    "\n",
    "##### c) The eigenvalues of $X^\\intercal X$ are all positive, and hence so is its trace\n",
    "\n",
    "Let $\\lambda$ be any eigenvalue of $X^\\intercal X$ with corresponding eigenvector $v\\in\\mathbb{R}^p$.\n",
    "\n",
    "Then, \n",
    "\\begin{eqnarray}\n",
    "v^\\intercal X^\\intercal Xv \\ &=& \\ v^\\intercal \\lambda v \\\\\n",
    "\\ \\\\\n",
    "&=& \\lambda v^\\intercal v \\\\\n",
    "\\ \\\\\n",
    "&=& \\lambda \\|v\\|_2^2 > 0\n",
    "\\end{eqnarray}\n",
    "\n",
    "The equality is strict due to $X^\\intercal X$ being positive semi-definite. Thus, all eigenvalues of $X^\\intercal X$ are positive.\n",
    "\n",
    "Now, since $X^\\intercal X = P\\Sigma P^{-1}$ (Jordan Canonical Form) we can conclude that the trace is positive:\n",
    "\\begin{eqnarray}\n",
    "tr(X^\\intercal X) &=& tr(P\\Sigma P^{-1})\\\\\n",
    "\\ \\\\\n",
    "&=& tr(\\Sigma PP^{-1}) \\\\ \n",
    "\\ \\\\\n",
    "&=& tr(\\Sigma) > 0\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "#### Consider the unconstrained least-squares problem with full-rank $X$ : $$ \\min_{\\theta} \\frac{1}{2} \\|y-X\\theta\\|_2^2,$$ Show that Newton's method applied to the this problem converges in one step.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{1}{2} \\|y-X\\theta\\|_2^2 &=& \\frac{1}{2}(y-X\\theta)^\\intercal(y-X\\theta)\\\\\n",
    "\\ \\\\\n",
    "&=& \\frac{1}{2} \\left( y^\\intercal - (X\\theta)^\\intercal) \\right) (y-X\\theta) \\\\\n",
    "\\ \\\\\n",
    "&=& \\frac{1}{2} \\left( \\|y\\|_2^2 - 2(X\\theta)^\\intercal y + \\theta^\\intercal X^\\intercal X\\theta \\right)\n",
    "\\end{eqnarray}\n",
    "Then, to minimize for $\\theta$ we take the gradient with respect to the parameters $\\theta$ and set to zero:\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial}{\\partial\\theta} \\frac{1}{2} \\left( \\|y\\|_2^2 - 2(X\\theta)^\\intercal y + \\theta^\\intercal X^\\intercal X\\theta \\right) &=& -X^\\intercal y + X^\\intercal X \\theta \\\\\n",
    "\\ \\\\\n",
    "-X^\\intercal y + X^\\intercal X \\theta &=& 0 \\\\\n",
    "\\ \\\\\n",
    "\\theta &=& (X^\\intercal X)^{-1} X^\\intercal y\n",
    "\\end{eqnarray}\n",
    "\n",
    "Thus, by taking this single step, we can minimize the Unconstrained Linear Squares loss. $\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "#### a.) Consider a ficticious league with 3 teams and make up 5 match-ups among the teams in your league where each team plays one of the other teams at least once. Write the model in the form:\n",
    "$$y = X\\beta + \\epsilon$$\n",
    "#### For your ficticious league, show that the null-space of $X$ is the span of the vector $(1,1,1,0)^\\intercal$. Use this to conclude that, for this model, the null-space of the corresponding $X$ from the NBA data is the span of the vector $\\gamma = (1,1,1,\\cdots,1,0)^\\intercal$\n",
    "\n",
    "Let, \n",
    "$$X = \\left[\\begin{array}{cccc} 1 & -1 & 0 & 1 \\\\ -1 & 0 & 1 & 1 \\\\ 0 & 1 & -1 & 1 \\\\ 0 & -1 & 1 & 1 \\\\ 1 & 0 & -1 & 1 \\end{array}\\right]$$\n",
    "\n",
    "We will determine the null-space by finding vectors $v\\in\\mathbb{R}^4$ such that $Xv = 0$.\n",
    "\n",
    "The matrix $X$ multiplied by the array $\\beta = \\left[\\theta_1 , \\theta_2 , \\theta_3 , \\epsilon\\right]^\\intercal$ admits the following system of equations.\n",
    "\\begin{eqnarray} \n",
    "\\theta_1 &-& \\theta_2 &\\ & \\ &+& \\epsilon &=& 0 \\\\\n",
    "-\\theta_1 &\\ &  &+& \\theta_3 &+& \\epsilon &=& 0 \\\\\n",
    " &\\ & \\theta_2 &-& \\theta_3 &+& \\epsilon &=& 0 \\\\\n",
    " &-& \\theta_2 &+& \\theta_3 &+& \\epsilon &=& 0 \\\\\n",
    "\\theta_1 &\\ & \\ &-& \\theta_3 &+& \\epsilon &=& 0 \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "Adding equations 3 and 4 gives, $2\\epsilon = 0 \\ \\Rightarrow \\ \\epsilon=0$\n",
    "\n",
    "Then, when observing all other equations, it follows that $\\theta_1 = \\theta_2 = \\theta_3$. Thus, the span of the null-space of our reduced $X$ is $\\left[1,1,1,0\\right]^\\intercal$. This extends to the full problem, where we can conclude that the null-space is spanned by $\\left[1,1,1,\\cdots,1,0\\right]^\\intercal \\ \\blacksquare$\n",
    "\n",
    "\n",
    "#### b.) Consider the problem \n",
    "$$\\min_{\\beta}\\frac{1}{2} \\|y-X\\beta\\|_2^2,$$\n",
    "#### where $X$ and $\\beta$ are defined as above. Suppose $\\hat{\\beta}$ minimizes the given objective function. Show that this solution is not unique.\n",
    "\n",
    "Consider any vector $v\\in\\mathbb{R}^4$ in the null-space of X. Then we can conclude that the vector $m = [\\hat{\n",
    "\\beta} + v]$ is also a minimizing solution to the unconstrained least squares problem. This follows from applying the objective to this vector $m$. That is:\n",
    "\n",
    "$$\\boxed{\\mathcal{L}(m) = \\frac{1}{2}\\|y-Xm\\|^2_2 = \\frac{1}{2}\\|y-X\\left[\\hat{\\beta} + v\\right]\\|_2^2 = \\frac{1}{2}\\|y-X\\hat{\\beta}\\|^2_2 \\ }$$\n",
    "\n",
    "Since $\\hat{\\beta}$ minimizes the objective, so does $m$. $\\blacksquare$\n",
    "\n",
    "\n",
    "#### c.) To identify a unique solution, we solve the constrained least-squares problem \n",
    "$$\\min_{\\beta} \\frac{1}{2} \\|y - X\\beta\\|_2^2 \\text{ s.t } \\gamma^\\intercal\\beta = 0$$\n",
    "#### Write down the Lagrangian function $L(\\beta,\\lambda)$ for this constrained optimization function.\n",
    "\n",
    "The Lagrangian of the the constrained least-squares problem is simply: \n",
    "\n",
    "$$\\boxed{L(\\beta,\\lambda) = \\frac{1}{2}\\|y-X\\beta\\|_2^2 + \\lambda\\left(\\gamma^\\intercal\\beta\\right)}$$\n",
    "\n",
    "$\\blacksquare$\n",
    "\n",
    "#### d.) The normal equations identify the solution of the unconstrained least-squares problem. The corresponding equations for the constrained case are: \n",
    "$$\\nabla_{\\beta} L(\\beta,\\lambda) = 0$$ $$\\nabla_{\\lambda} L(\\beta,\\lambda) = 0$$\n",
    "#### Show that solving this system of equations is equivalent to solving a linear system of the form\n",
    "$$ \\left[\\begin{array}{c|c} A & b^\\intercal \\\\ \\hline b & 0 \\end{array}\\right] \\left[ \\begin{array}{c} \\beta \\\\ \\hline \\lambda \\end{array}\\right] = \\left[ \\begin{array}{c} z \\\\ \\hline 0 \\end{array}\\right]$$\n",
    "#### where $A,b\\text{ and }z$ depend on $X,y\\text{ and } \\gamma$.\n",
    "\n",
    "First we expand the Lagrangian: $L(\\beta,\\lambda) = \\frac{1}{2}\\left(y-X\\beta\\right)^\\intercal\\left(y-X\\beta\\right) + \\lambda\\left(\\gamma^\\intercal\\beta\\right)$ Then:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\ \\\\\n",
    "\\nabla_{\\beta} L(\\beta,\\lambda) &=& -y^\\intercal X + X^\\intercal X \\beta &+& \\lambda\\gamma^\\intercal &=& 0 \\\\\n",
    "\\text{  and  } \\\\\n",
    "\\nabla_{\\lambda} L(\\beta, \\lambda) &=& &\\ &\\gamma^\\intercal\\beta &=& 0 \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "This set of equations can be re-composed into the corresponding matrix equation by setting the following:\n",
    "\\begin{eqnarray}\n",
    "\\ \\\\\n",
    "A &=& X^\\intercal X \\\\\n",
    "\\ \\\\\n",
    "b &=& \\gamma^\\intercal \\\\\n",
    "\\ \\\\\n",
    "z &=& y^\\intercal X\n",
    "\\end{eqnarray}\n",
    "$\\blacksquare$\n",
    "\n",
    "#### e.) Solve the ranking problem using contrained least-squares and the model of Eq. 4 applied to the data in $\\texttt{nba2014-2015-seasonWL-WL-data.json}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create training data arrays\n",
    "json_data=open('nba2014-2015-seasonWL-WL-data.json').read()\n",
    "\n",
    "data = json.loads(json_data)\n",
    "teams = ['atl', 'bkn', 'bos', 'cha', 'chi', 'cle', 'dal', 'den', 'det','gsw', 'hou', 'ind', 'lac', 'lal', 'mem', 'mia', 'mil', 'min','nop', 'nyk', 'okc', 'orl', 'phi', 'phx', 'por', 'sac', 'sas','tor', 'uta', 'was']\n",
    "X = np.zeros([1408,31])\n",
    "X[:,-1] = 1\n",
    "y = np.zeros(1408)\n",
    "game_counter = 0\n",
    "for date in data.keys():\n",
    "    for game in data[date]:\n",
    "        X[game_counter, teams.index(game['home_team'])] = 1\n",
    "        X[game_counter, teams.index(game['away_team'])] = -1\n",
    "        y[game_counter] = int(game['home_score']) - int(game['away_score'])\n",
    "        game_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# solve with normal equations\n",
    "beta = np.dot(np.linalg.inv(np.dot(X.T,X)) , np.dot(X.T,y) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'gsw'),\n",
       " (2, 'lac'),\n",
       " (3, 'sas'),\n",
       " (4, 'hou'),\n",
       " (5, 'cle'),\n",
       " (6, 'atl'),\n",
       " (7, 'chi'),\n",
       " (8, 'tor'),\n",
       " (9, 'dal'),\n",
       " (10, 'por'),\n",
       " (11, 'uta'),\n",
       " (12, 'bos'),\n",
       " (13, 'was'),\n",
       " (14, 'mem'),\n",
       " (15, 'phx'),\n",
       " (16, 'okc'),\n",
       " (17, 'nop'),\n",
       " (18, 'ind'),\n",
       " (19, 'det'),\n",
       " (20, 'mia'),\n",
       " (21, 'den'),\n",
       " (22, 'cha'),\n",
       " (23, 'bkn'),\n",
       " (24, 'sac'),\n",
       " (25, 'orl'),\n",
       " (26, 'mil'),\n",
       " (27, 'lal'),\n",
       " (28, 'min'),\n",
       " (29, 'phi'),\n",
       " (30, 'nyk')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort teams based on relative strength (beta coefficient for that team)\n",
    "ranked_teams = list(zip(np.arange(1,31),[team for (rnk,team) in sorted(zip(beta[:-1],teams), key=lambda pair: pair[0])[::-1]]))\n",
    "ranked_teams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home team weighting parameter: 2.2776078826361927\n"
     ]
    }
   ],
   "source": [
    "# Print out the home_team weighting parameter\n",
    "print(\"Home team weighting parameter: {}\".format(beta[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "#### Let $(y_i)_{i=1}^{n}$ be i.i.d. samples from a random variable with bounded variance. Show that the sample mean is an unbiased estimator with variance that tends to zero asymptotically as $n$ tends to infinity.\n",
    "\n",
    "Let $\\bar{y}$ be the sample mean (i.e. $\\bar{y} = \\frac{1}{n} \\Sigma_{i=1}^n y_i$).\n",
    "\n",
    "Then,\n",
    "\\begin{eqnarray}\n",
    "E[ \\ \\bar{y} \\ ] &=& E\\left[\\frac{1}{n} \\Sigma_{i=1}^n y_i\\right] \\\\\n",
    "\\ \\\\\n",
    "&=& \\frac{1}{n} E\\left[\\Sigma_{i=1}^n y_i\\right] = \\frac{1}{n} \\Sigma_{i=1}^n E[ \\ y_i \\ ] \\\\\n",
    "\\ \\\\\n",
    "&=& \\frac{1}{n} n E[ \\ y_i \\ ] = E[ \\ y_i \\ ] = \\mu\n",
    "\\end{eqnarray}\n",
    "\n",
    "Thus the sample mean is an unbiased estimator of the true mean.\n",
    "\n",
    "Now, the variance of $\\bar{y}$ is as follows:\n",
    "\\begin{eqnarray}\n",
    "var( \\ \\bar{y} \\ ) &=& var\\left( \\ \\frac{1}{n} \\Sigma_{i=1}^{n} y_i \\ \\right) \\\\\n",
    "\\ \\\\\n",
    "&=& \\frac{1}{n^2} var\\left( \\ \\Sigma_{i=1}^n y_i \\ \\right) = \\frac{1}{n^2} \\Sigma_{i=1}^n var( \\ y_i \\ )\\\\\n",
    "\\ \\\\\n",
    "&=& \\frac{n}{n^2} var( \\ y_i \\ ) = \\frac{\\sigma^2}{n}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Thus, as $n\\to\\infty$, $var( \\ \\bar{y} \\ ) \\to 0 \\ \\ \\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "#### Let $\\hat{\\theta}_u$ be an unbiased estimator of a deterministic, unknown, parameter $\\theta_0$. Define a family of biased estimators $\\hat{\\theta}_b = (1+\\alpha)\\hat{\\theta}_b$. Show that the range for $\\alpha$ for which the MSE of $\\hat{\\theta}_b$ is smaller than that of $\\hat{\\theta}_u$ is:\n",
    "$$-2 < - \\frac{2MSE(\\hat{\\theta}_u)}{MSE(\\hat{\\theta}_u)+\\theta_0^2} < \\alpha < 0$$\n",
    "\n",
    "\n",
    "Equation 3.23 from the textbook tells us:\n",
    "\\begin{eqnarray}\n",
    "MSE\\left(\\hat{\\theta_b}\\right) &=& (1+\\alpha)^2 MSE\\left(\\hat{\\theta_u}\\right) + \\alpha^2 \\theta_0^2 \\\\\n",
    "\\ \\\\\n",
    "&=& \\left(1+2\\alpha + \\alpha^2\\right)MSE\\left(\\hat{\\theta_u}\\right) + \\alpha^2 \\theta_0^2 \\\\\n",
    "\\ \\\\\n",
    "&=& MSE\\left(\\hat{\\theta_u}\\right) + 2\\alpha MSE\\left(\\hat{\\theta_u}\\right) + \\alpha^2 MSE\\left(\\hat{\\theta_u}\\right) + \\alpha^2\\theta_0^2\\\\\n",
    "\\ \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "Thus, $MSE\\left(\\hat{\\theta_b}\\right) < MSE\\left(\\hat{\\theta_u}\\right)$  if:\n",
    "+ $2\\alpha MSE\\left(\\hat{\\theta_u}\\right) + \\alpha^2 MSE\\left(\\hat{\\theta_u}\\right) + \\alpha^2\\theta_0^2$\n",
    "+ $(1+\\alpha)^2 < 1$.\n",
    "\n",
    "\n",
    "So,\n",
    "\\begin{eqnarray}\n",
    "2\\alpha MSE\\left(\\hat{\\theta_u}\\right) + \\alpha^2 MSE\\left(\\hat{\\theta_u}\\right) + \\alpha^2\\theta_0^2 &<& 0 \\\\\n",
    "\\ \\\\\n",
    "2\\alpha MSE\\left(\\hat{\\theta_u}\\right) < \\alpha^2 \\left(MSE\\left(\\hat{\\theta_u}\\right) + \\theta_0^2\\right) &<& 0\\\\\n",
    "\\ \\\\\n",
    "-\\frac{2 MSE\\left(\\hat{\\theta_u}\\right)}{\\left(MSE\\left(\\hat{\\theta_u}\\right) + \\theta_0^2\\right)} < \\alpha &<& 0 \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "Now, $(1+\\alpha)^2 < 1$ if $-2 < \\alpha < 0$. So we constrain the above inequality accordingly. That is:\n",
    "\n",
    "$$ -2 < -\\frac{2 MSE\\left(\\hat{\\theta_u}\\right)}{\\left(MSE\\left(\\hat{\\theta_u}\\right) + \\theta_0^2\\right)} < \\alpha < 0$$\n",
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6\n",
    "#### Show that for the linear regression model\n",
    "$$ y = X\\theta + \\epsilon $$\n",
    "#### the a-posteriori probability density function $p_{\\theta}(\\theta|y)$ is Gaussian if the prior $p_{\\theta}(\\theta)\\sim\\mathcal{N}(\\theta_0,\\Sigma_0)$ and $\\epsilon$ follows a $\\mathcal{N}(0,\\Sigma_{\\epsilon})$ distribution. Compute the mean and covariance matrix of the posterior distribution.\n",
    "\n",
    "We generally seek to show that $p_{\\theta}(\\theta|y)$ is proportional to $\\exp\\left(-\\frac{1}{2}(\\theta - \\mu)^\\intercal \\Lambda (\\theta - \\mu)\\right)$ That is, the term within the exponential needs to be of the form (after expansion) $\\theta^\\intercal\\Lambda\\theta - 2\\theta^\\intercal\\Lambda\\mu + \\mu^\\intercal\\Lambda\\mu$ where the last term is constant with respect to $\\theta$.\n",
    "\n",
    "Now, \n",
    "\\begin{eqnarray}\n",
    "p_{\\theta}(\\theta|y) &\\propto& p_{\\theta}(y \\ | \\ \\theta) \\ p_{\\theta}(\\theta) \\\\\n",
    "\\ \\\\\n",
    "&\\propto& \\exp\\left(-\\frac{1}{2} (y - X\\theta)^\\intercal \\Sigma_{\\epsilon}^{-1} (y - X\\theta)\\right)\\times\\exp\\left(-\\frac{1}{2} (\\theta - \\theta_0)^\\intercal \\Sigma_0^{-1} (\\theta - \\theta_0)\\right)\\\\\n",
    "\\ \\\\\n",
    "&=& exp\\left( -\\frac{1}{2} \\left[ (y - X\\theta)^\\intercal \\Sigma_{\\epsilon}^{-1} (y - X\\theta) + (\\theta - \\theta_0)^\\intercal \\Sigma_0^{-1} (\\theta - \\theta_0) \\right] \\right)\n",
    "\\end{eqnarray}\n",
    "\n",
    "Then after expanding the term within the exponential we have:\n",
    "\n",
    "$$\\theta^\\intercal\\Sigma_0^{-1}\\theta - \\theta^\\intercal\\Sigma_0^{-1}\\theta_0 - \\theta_0^\\intercal\\Sigma_0^{-1}\\theta + \\theta_0^\\intercal\\Sigma_0^{-1}\\theta_0 + y^\\intercal\\Sigma_{\\epsilon}^{-1}y - \\theta^\\intercal X^\\intercal\\Sigma_{\\epsilon}^{-1}y - y^\\intercal\\Sigma_{\\epsilon}^{-1}X\\theta + \\theta^\\intercal X^\\intercal\\Sigma_{\\epsilon}^{-1}X\\theta$$\n",
    "\n",
    "And then after gathering like terms we have:\n",
    "\n",
    "$$\\theta^\\intercal \\left(\\Sigma_0^{-1} + X^\\intercal \\Sigma_{\\epsilon}^{-1}X\\right) \\theta - 2 \\theta^\\intercal \\left(\\Sigma_0^{-1}\\theta_0 - X^\\intercal\\Sigma_{\\epsilon}^{-1}y\\right) + \\left(\\theta_0^\\intercal \\Sigma_0^{-1} \\theta_0 + y^\\intercal \\Sigma_{\\epsilon}^{-1} y\\right)$$\n",
    "\n",
    "Thus our posterior distribution has the general Gaussian form as written above. We can therefore conclude that our posterior distribution is therefore Gaussian. We can use the first two terms of the general form to help deduce what the mean and covariance matrix of the posterior. That is:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\theta^\\intercal \\Lambda \\theta &=& \\theta^\\intercal \\left(\\Sigma_0^{-1} + X^\\intercal \\Sigma_{\\epsilon}^{-1}X\\right) \\theta \\\\\n",
    "\\ \\\\\n",
    "2\\theta^\\intercal\\Lambda\\mu &=& 2 \\theta^\\intercal \\left(\\Sigma_0^{-1}\\theta_0 - X^\\intercal\\Sigma_{\\epsilon}^{-1}y\\right)\\\\\n",
    "\\ \\\\\n",
    "\\Rightarrow \\ \\ \\ \\ \\Lambda &=& \\Sigma_0^{-1} + X^\\intercal \\Sigma_{\\epsilon}^{-1}X\\\\ \n",
    "\\ \\\\\n",
    "\\Rightarrow \\ \\ \\ \\ \\Lambda\\mu &=& \\left(\\Sigma_0^{-1}\\theta_0 - X^\\intercal\\Sigma_{\\epsilon}^{-1}y\\right) \\\\\n",
    "\\ \\\\\n",
    "\\mu &=& \\Lambda^{-1}\\left(\\Sigma_0^{-1}\\theta_0 - X^\\intercal\\Sigma_{\\epsilon}^{-1}y\\right)\\\\ \n",
    "\\ \\\\\n",
    "\\mu &=& \\left(\\Sigma_0^{-1} + X^\\intercal \\Sigma_{\\epsilon}^{-1}X\\right)^{-1}\\left(\\Sigma_0^{-1}\\theta_0 - X^\\intercal\\Sigma_{\\epsilon}^{-1}y\\right)\n",
    "\\end{eqnarray}\n",
    "\n",
    "Thus, the posterior $p_{\\theta}(\\theta|y)$ is distributed as $$\\boxed{\\mathcal{N}\\left( \\left(\\Sigma_0^{-1} + X^\\intercal \\Sigma_{\\epsilon}^{-1}X\\right)^{-1}\\left(\\Sigma_0^{-1}\\theta_0 - X^\\intercal\\Sigma_{\\epsilon}^{-1}y\\right) , \\left(\\Sigma_0^{-1} + X^\\intercal \\Sigma_{\\epsilon}^{-1}X\\right)^{-1} \\right)}$$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
